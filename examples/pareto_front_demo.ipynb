{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2fb6de8",
   "metadata": {},
   "source": [
    "# Pareto Front Analysis for Soil Gas Flux Parameter Optimization\n",
    "\n",
    "This notebook demonstrates how to use Pareto front analysis to find the optimal balance between measurement uncertainty and model fit quality in soil gas flux calculations.\n",
    "\n",
    "## Concept\n",
    "\n",
    "When analyzing chamber CO2 measurements with the H-M model, we face a trade-off:\n",
    "- **Lower uncertainty range** (more precise estimates) vs. **Better model fit** (higher log-likelihood)\n",
    "- Different combinations of deadband/cutoff parameters provide different points on this trade-off curve\n",
    "- The Pareto front identifies the \"efficient frontier\" where you cannot improve one objective without worsening the other\n",
    "\n",
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e3c4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from soilgasflux_fcs import HM_model, json_reader\n",
    "import xarray as xr\n",
    "\n",
    "def find_pareto_front(x, y, maximize_x=False, maximize_y=False):\n",
    "    \"\"\"\n",
    "    Find the Pareto front for two objectives\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x, y : array-like\n",
    "        Values of the two objectives (uncertainty range vs loglikelihood)\n",
    "    maximize_x, maximize_y : bool\n",
    "        Whether to maximize (True) or minimize (False) each objective\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pareto_indices : ndarray\n",
    "        Indices of points on the Pareto front\n",
    "    \"\"\"\n",
    "    # Copy arrays to avoid modifying originals\n",
    "    x_values = np.copy(x)\n",
    "    y_values = np.copy(y)\n",
    "    \n",
    "    # Convert maximization to minimization\n",
    "    if maximize_x:\n",
    "        x_values = -x_values\n",
    "    if maximize_y:\n",
    "        y_values = -y_values\n",
    "    \n",
    "    points = np.column_stack((x_values, y_values))\n",
    "    pareto_indices = []\n",
    "    \n",
    "    for i, point in enumerate(points):\n",
    "        if np.isnan(point).any():\n",
    "            continue\n",
    "            \n",
    "        dominated = False\n",
    "        for j, other_point in enumerate(points):\n",
    "            if i != j and not np.isnan(other_point).any():\n",
    "                # Check if other_point dominates point (smaller is better)\n",
    "                if (all(other_point <= point) and any(other_point < point)):\n",
    "                    dominated = True\n",
    "                    break\n",
    "        \n",
    "        if not dominated:\n",
    "            pareto_indices.append(i)\n",
    "    \n",
    "    return np.array(pareto_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac50b29d",
   "metadata": {},
   "source": [
    "## Example Analysis with Synthetic Data\n",
    "\n",
    "Let's demonstrate the concept with a synthetic dataset where we know the \"true\" optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3004dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic MCMC results for demonstration\n",
    "# In practice, these would come from actual MCMC runs with different deadband/cutoff combinations\n",
    "\n",
    "np.random.seed(42)\n",
    "n_parameter_combinations = 50\n",
    "\n",
    "# Simulate uncertainty range and log-likelihood for different parameter combinations\n",
    "# Create realistic trade-off: as uncertainty decreases, log-likelihood tends to get worse\n",
    "base_uncertainty = np.random.uniform(0.1, 2.0, n_parameter_combinations)\n",
    "base_loglik = np.random.uniform(-30, -5, n_parameter_combinations)\n",
    "\n",
    "# Add correlation: lower uncertainty often means worse model fit\n",
    "uncertainty_range = base_uncertainty + 0.3 * np.random.normal(0, 1, n_parameter_combinations)\n",
    "loglikelihood = base_loglik - 5 * (1 / base_uncertainty) + 2 * np.random.normal(0, 1, n_parameter_combinations)\n",
    "\n",
    "# Normalize for visualization\n",
    "norm_uncertainty = (uncertainty_range - np.min(uncertainty_range)) / (np.max(uncertainty_range) - np.min(uncertainty_range))\n",
    "norm_loglik = (loglikelihood - np.min(loglikelihood)) / (np.max(loglikelihood) - np.min(loglikelihood))\n",
    "\n",
    "print(f\"Generated {n_parameter_combinations} parameter combinations\")\n",
    "print(f\"Uncertainty range: {uncertainty_range.min():.3f} - {uncertainty_range.max():.3f}\")\n",
    "print(f\"Log-likelihood range: {loglikelihood.min():.1f} - {loglikelihood.max():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39f577e",
   "metadata": {},
   "source": [
    "## Find Pareto Front and Optimal Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2727c85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Pareto front\n",
    "# We want to minimize both uncertainty (x) and negative log-likelihood (y)\n",
    "pareto_indices = find_pareto_front(norm_uncertainty, norm_loglik, maximize_x=False, maximize_y=False)\n",
    "\n",
    "print(f\"Found {len(pareto_indices)} points on Pareto front\")\n",
    "\n",
    "# Find best compromise solution (closest to origin in normalized space)\n",
    "distances = np.sqrt(norm_uncertainty[pareto_indices]**2 + norm_loglik[pareto_indices]**2)\n",
    "best_pareto_idx = pareto_indices[np.argmin(distances)]\n",
    "\n",
    "print(f\"\\nBest compromise solution:\")\n",
    "print(f\"  Index: {best_pareto_idx}\")\n",
    "print(f\"  Uncertainty range: {uncertainty_range[best_pareto_idx]:.3f}\")\n",
    "print(f\"  Log-likelihood: {loglikelihood[best_pareto_idx]:.1f}\")\n",
    "print(f\"  Distance from origin: {distances[np.argmin(distances)]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423141c4",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36d364e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Original scale\n",
    "ax1.scatter(uncertainty_range, loglikelihood, s=30, c='gray', alpha=0.6, label='All combinations')\n",
    "ax1.scatter(uncertainty_range[pareto_indices], loglikelihood[pareto_indices], \n",
    "           s=50, c='red', alpha=0.8, label='Pareto front', zorder=5)\n",
    "ax1.scatter(uncertainty_range[best_pareto_idx], loglikelihood[best_pareto_idx], \n",
    "           s=100, c='blue', marker='*', label='Best compromise', zorder=10)\n",
    "\n",
    "ax1.set_xlabel('Uncertainty Range')\n",
    "ax1.set_ylabel('Log-likelihood')\n",
    "ax1.set_title('Pareto Front Analysis (Original Scale)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Normalized scale with distance visualization\n",
    "ax2.scatter(norm_uncertainty, norm_loglik, s=30, c='gray', alpha=0.6, label='All combinations')\n",
    "ax2.scatter(norm_uncertainty[pareto_indices], norm_loglik[pareto_indices], \n",
    "           s=50, c='red', alpha=0.8, label='Pareto front', zorder=5)\n",
    "ax2.scatter(norm_uncertainty[best_pareto_idx], norm_loglik[best_pareto_idx], \n",
    "           s=100, c='blue', marker='*', label='Best compromise', zorder=10)\n",
    "\n",
    "# Draw line from origin to best solution\n",
    "ax2.plot([0, norm_uncertainty[best_pareto_idx]], [0, norm_loglik[best_pareto_idx]], \n",
    "         'b--', alpha=0.7, label='Distance to origin')\n",
    "ax2.scatter([0], [0], s=50, c='black', marker='x', label='Origin (ideal)')\n",
    "\n",
    "ax2.set_xlabel('Normalized Uncertainty Range')\n",
    "ax2.set_ylabel('Normalized Log-likelihood')\n",
    "ax2.set_title('Pareto Front Analysis (Normalized Scale)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim(0, 1.1)\n",
    "ax2.set_ylim(0, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6349ee67",
   "metadata": {},
   "source": [
    "## Practical Application\n",
    "\n",
    "In real applications, you would:\n",
    "\n",
    "1. **Run MCMC analysis** for multiple deadband/cutoff combinations\n",
    "2. **Calculate metrics** for each combination:\n",
    "   - Uncertainty range: `np.quantile(dcdt_samples, 0.84) - np.quantile(dcdt_samples, 0.16)`\n",
    "   - Log-likelihood: `logprob_samples.mean()`\n",
    "3. **Apply Pareto analysis** to find optimal parameters\n",
    "4. **Select best solution** based on your priorities (precision vs. model fit)\n",
    "\n",
    "### Example with Real Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2f17b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example workflow with netCDF output from multiprocessing\n",
    "# This shows how you would apply Pareto analysis to real MCMC results\n",
    "\n",
    "def analyze_mcmc_results(ds, time_idx=0):\n",
    "    \"\"\"\n",
    "    Apply Pareto front analysis to MCMC results from netCDF dataset\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ds : xarray.Dataset\n",
    "        Dataset with MCMC results including 'dcdt(HM)' and 'logprob(HM)'\n",
    "    time_idx : int\n",
    "        Time index to analyze\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with optimal parameters and metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract data for specific time\n",
    "    time_data = ds.isel(time=time_idx)\n",
    "    \n",
    "    # Calculate uncertainty range (68% confidence interval)\n",
    "    dcdt_q84 = time_data['dcdt(HM)'].quantile(0.84, dim='MC')\n",
    "    dcdt_q16 = time_data['dcdt(HM)'].quantile(0.16, dim='MC')\n",
    "    uncertainty_range = dcdt_q84 - dcdt_q16\n",
    "    \n",
    "    # Get log-likelihood (average over MC samples)\n",
    "    loglikelihood = -time_data['logprob(HM)'].mean(dim='MC')\n",
    "    \n",
    "    # Flatten for Pareto analysis\n",
    "    flat_uncertainty = uncertainty_range.values.flatten()\n",
    "    flat_loglik = loglikelihood.values.flatten()\n",
    "    \n",
    "    # Remove NaN values\n",
    "    valid_mask = ~(np.isnan(flat_uncertainty) | np.isnan(flat_loglik))\n",
    "    flat_uncertainty = flat_uncertainty[valid_mask]\n",
    "    flat_loglik = flat_loglik[valid_mask]\n",
    "    \n",
    "    # Normalize\n",
    "    norm_uncertainty = (flat_uncertainty - np.min(flat_uncertainty)) / (np.max(flat_uncertainty) - np.min(flat_uncertainty))\n",
    "    norm_loglik = (flat_loglik - np.min(flat_loglik)) / (np.max(flat_loglik) - np.min(flat_loglik))\n",
    "    \n",
    "    # Find Pareto front\n",
    "    pareto_indices = find_pareto_front(norm_uncertainty, norm_loglik, maximize_x=False, maximize_y=False)\n",
    "    \n",
    "    # Find best compromise\n",
    "    distances = np.sqrt(norm_uncertainty[pareto_indices]**2 + norm_loglik[pareto_indices]**2)\n",
    "    best_pareto_local_idx = np.argmin(distances)\n",
    "    best_global_idx = np.where(valid_mask)[0][pareto_indices[best_pareto_local_idx]]\n",
    "    \n",
    "    # Convert back to cutoff/deadband coordinates\n",
    "    coords = np.unravel_index(best_global_idx, uncertainty_range.shape)\n",
    "    optimal_cutoff = time_data.coords['cutoff'][coords[0]].values\n",
    "    optimal_deadband = time_data.coords['deadband'][coords[1]].values\n",
    "    \n",
    "    return {\n",
    "        'optimal_cutoff': optimal_cutoff,\n",
    "        'optimal_deadband': optimal_deadband,\n",
    "        'optimal_uncertainty': flat_uncertainty[pareto_indices[best_pareto_local_idx]],\n",
    "        'optimal_loglik': flat_loglik[pareto_indices[best_pareto_local_idx]],\n",
    "        'pareto_points': len(pareto_indices),\n",
    "        'total_combinations': len(flat_uncertainty)\n",
    "    }\n",
    "\n",
    "# Example usage (commented out since we don't have real data loaded)\n",
    "# ds = xr.open_dataset('path/to/mcmc_results.nc')\n",
    "# optimal_params = analyze_mcmc_results(ds, time_idx=0)\n",
    "# print(f\"Optimal parameters: cutoff={optimal_params['optimal_cutoff']}, deadband={optimal_params['optimal_deadband']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127d37ce",
   "metadata": {},
   "source": [
    "## Key Benefits\n",
    "\n",
    "1. **Objective Parameter Selection**: Instead of arbitrarily choosing deadband/cutoff, find the mathematically optimal balance\n",
    "\n",
    "2. **Quality Control**: Identify measurements where no good trade-off exists (all solutions are dominated)\n",
    "\n",
    "3. **Measurement Optimization**: Understand how different measurement windows affect the precision-accuracy trade-off\n",
    "\n",
    "4. **Reproducible Analysis**: Consistent, algorithmic approach to parameter selection across studies\n",
    "\n",
    "## Implementation Notes\n",
    "\n",
    "- The analysis assumes you want to **minimize both** uncertainty and negative log-likelihood\n",
    "- Normalization is important when objectives have different scales\n",
    "- The \"best\" point can be selected using different criteria (closest to origin, weighted distance, etc.)\n",
    "- Consider domain expertise when interpreting results - sometimes a slightly dominated solution might be preferable for practical reasons"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
